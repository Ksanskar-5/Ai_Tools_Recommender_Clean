import os
import json
import logging
import re
import subprocess
from typing import List, Dict, Any

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

from backend.utils.vectorizer import semantic_search as vs_semantic_search
from backend.utils.simple_parse import simple_parse as simple_parse_func
from backend.utils.db import get_all_ai

# Environment / model selection
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:3b")
# legacy quick model kept for fallback use if needed
QUICK_MODEL = os.getenv("QUICK_MODEL", "mistral:7b-instruct")
TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", "120"))

logger = logging.getLogger(__name__)
BASE_DATA_PATH = os.environ.get("AI_RECOMMENDER_DATA", "data")
DATA_PATH = os.path.join(BASE_DATA_PATH, "processed", "ai_database_clean.csv")
INDEX_DIR = os.path.join("models")
INDEX_PATH = os.path.join(INDEX_DIR, "vector_store.faiss")
EMBEDDINGS_PATH = os.path.join(INDEX_DIR, "embeddings.npy")

RECOMMENDATION_KEYS = [
    "ai_id", "Name", "Company", "Category", "Subcategory",
    "Input Type", "Output Type", "Task Description", "Cost",
    "Ease of Use", "Integration", "Languages", "Rating",
    "Popularity", "Accuracy", "Speed", "Training Domain",
    "Link", "score", "Reasoning"
]


# -----------------------------------------------------
# Normalization
# -----------------------------------------------------
def normalize_records(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    def clean(value):
        return "" if value is None else str(value).strip().title()

    normalized = []
    for i, r in enumerate(records):
        nr = {k: r.get(k, "") for k in RECOMMENDATION_KEYS}
        nr["ai_id"] = r.get("ai_id") or r.get("ID") or i + 1

        for field in ["Name", "Company", "Category", "Subcategory",
                      "Integration", "Languages", "Input Type", "Output Type"]:
            nr[field] = clean(nr.get(field, ""))

        nr["Link"] = (r.get("Link") or "").strip()
        try:
            nr["Rating"] = float(r.get("Rating") or 0)
        except Exception:
            nr["Rating"] = 0.0

        try:
            nr["score"] = float(r.get("score") or 0)
        except Exception:
            nr["score"] = 0.0

        nr["Reasoning"] = r.get("Reasoning", "") or ""
        normalized.append(nr)

    return normalized


# -----------------------------------------------------
# LLM fallback (uses QUICK_MODEL)
# -----------------------------------------------------
def llm_fallback(query: str) -> List[Dict[str, Any]]:
    """
    Call local Ollama model as a fallback recommender. Returns a single-item list
    with a standardized dict for downstream processing.
    """
    prompt = (
        "You are an expert AI tool recommender. Output exactly one concise paragraph "
        "(<=40 words). Start with a short headline listing 1–3 top tools (comma-separated), "
        "then one brief sentence why they fit. No extra text, no questions.\n\n"
        f"Task: \"{query}\""
    )

    try:
        result = subprocess.run(
            ["ollama", "run", QUICK_MODEL, prompt],
            capture_output=True,
            text=True,
            timeout=TIMEOUT
        )

        # Prefer stdout but fall back to stderr for diagnostics
        output = (result.stdout or "").strip()
        if not output:
            output = (result.stderr or "").strip() or "No response from model."

        return [{
            "ai_id": 999,
            "Name": "LLM-Suggested",
            "Company": "Ollama Local",
            "Category": "General",
            "Task Description": output,
            "Cost": "Unknown",
            "Ease of Use": "3",
            "Integration": "N/A",
            "Rating": 4.0,
            "Link": "",
            "score": 0.6,
            "Reasoning": "Generated by LLM fallback"
        }]

    except subprocess.TimeoutExpired as te:
        logger.exception("llm_fallback timeout")
        return [{
            "ai_id": 0,
            "Name": "LLM-Fallback-Error",
            "Category": "Error",
            "Task Description": f"Timeout after {TIMEOUT} seconds.",
            "score": 0.0
        }]
    except Exception as e:
        logger.exception("llm_fallback error")
        return [{
            "ai_id": 0,
            "Name": "LLM-Fallback-Error",
            "Category": "Error",
            "Task Description": str(e),
            "score": 0.0
        }]


# -----------------------------------------------------
# Utility: robust JSON extraction from model output
# -----------------------------------------------------
def extract_json_from_text(text: str) -> Any:
    """
    Try to find a JSON array or object inside the raw model output.
    Returns parsed JSON or None on failure.
    """
    if not text:
        return None

    # common patterns: whole text is json, or contains a json block
    text = text.strip()

    # quick try: whole text parse
    try:
        return json.loads(text)
    except Exception:
        pass

    # regex to find first {...} or [...]
    patterns = [
        r"(\[.*\])",  # array
        r"(\{.*\})"   # object
    ]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL)
        if m:
            candidate = m.group(1)
            # try to balance braces/brackets if truncated (simple fix)
            # attempt parse, if fails, try trimming trailing characters
            for end_cut in range(0, 5):
                attempt = candidate[:-end_cut] if end_cut > 0 else candidate
                try:
                    return json.loads(attempt)
                except Exception:
                    continue
    return None


# -----------------------------------------------------
# Quick Recommendation (unchanged logic)
# -----------------------------------------------------
def recommend_ai(input_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
    input_text = (input_text or "").strip()
    if not input_text:
        return []

    # Simple parse
    try:
        parsed = simple_parse_func(input_text)
        if parsed:
            return normalize_records(parsed)
    except Exception:
        logger.debug("simple_parse_func failed", exc_info=True)

    # Semantic search
    try:
        results = vs_semantic_search(input_text, top_k=top_k)
        if results:
            return normalize_records(results)
    except Exception:
        logger.debug("vs_semantic_search failed in recommend_ai", exc_info=True)

    # Fallback
    return normalize_records(llm_fallback(input_text))

def deep_search_ai(input_text: str, top_k: int = 10) -> List[Dict[str, Any]]:

    print("\n================= DEEP SEARCH START =================")
    print("QUERY:", input_text)
    print("TOP_K:", top_k)
    print("=====================================================\n")

    input_text = (input_text or "").strip()
    if not input_text:
        print("EMPTY QUERY → RETURN []")
        return []

    try:
        # -------------------------------------------------------------
        # 1) SEMANTIC FIRST-STAGE
        # -------------------------------------------------------------
        print(">>> STEP 1: Semantic retrieval")
        try:
            candidates = vs_semantic_search(input_text, top_k=top_k) or []
        except Exception as e:
            print("Semantic search failed:", e)
            candidates = []

        print("Semantic candidates:", len(candidates))
        print(candidates, "\n")

        # -------------------------------------------------------------
        # 2) CSV + embeddings or TF-IDF
        # -------------------------------------------------------------
        if not candidates:
            print("\n>>> STEP 2: Loading CSV fallback…")
            import pandas as pd
            from pathlib import Path
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            from sentence_transformers import SentenceTransformer

            print("Reading CSV:", DATA_PATH)
            df = pd.read_csv(DATA_PATH).fillna("")
            texts = df["Task Description"].astype(str).tolist()

            # ---- embeddings available?
            if Path(EMBEDDINGS_PATH).exists():
                print("Found embeddings → using embedding search:", EMBEDDINGS_PATH)
                try:
                    emb_matrix = np.load(EMBEDDINGS_PATH)
                    print("Loaded embedding matrix:", emb_matrix.shape)

                    # get embedding model
                    try:
                        model = globals().get("get_embedding_model", None)
                        emb_model = model() if callable(model) else SentenceTransformer("all-MiniLM-L6-v2")
                        print("Using embedding model:", emb_model)
                    except Exception:
                        emb_model = SentenceTransformer("all-MiniLM-L6-v2")
                        print("Fallback embedding model: MiniLM")

                    q_emb = emb_model.encode(input_text, convert_to_numpy=True)
                    print("Query embedding shape:", q_emb.shape)

                    def normalize(a):
                        a = a.astype("float32")
                        norms = np.linalg.norm(a, axis=1, keepdims=True)
                        norms[norms == 0] = 1.0
                        return a / norms

                    M = normalize(emb_matrix)
                    qn = (q_emb.astype("float32") / (np.linalg.norm(q_emb) or 1.0)).reshape(1, -1)
                    sims = (M @ qn.T).squeeze()

                    print("Similarity scores computed.")
                    top_idx = np.argsort(-sims)[:top_k]
                    print("Top indices:", top_idx)

                    candidates = []
                    for i in top_idx:
                        row = df.iloc[int(i)].to_dict()
                        row["score"] = float(sims[int(i)])
                        candidates.append(row)

                except Exception as e:
                    print("Embedding pipeline failed:", e)
                    candidates = []

            # ---- TF-IDF fallback ----
            if not candidates:
                print("Using TF-IDF fallback…")
                try:
                    tfidf = TfidfVectorizer(stop_words="english")
                    matrix = tfidf.fit_transform(texts + [input_text])
                    qvec = matrix[-1]
                    sims = cosine_similarity(qvec, matrix[:-1])[0]
                    idxs = np.argsort(-sims)[:top_k]
                    print("TF-IDF top indices:", idxs)

                    candidates = []
                    for i in idxs:
                        row = df.iloc[int(i)].to_dict()
                        row["score"] = float(sims[int(i)])
                        candidates.append(row)

                except Exception as e:
                    print("TF-IDF failed:", e)
                    candidates = []

            if not candidates:
                print("NO CANDIDATES FOUND EVEN AFTER CSV → Using fallback LLM")
                return normalize_records(llm_fallback(input_text))

        print(">>> FIRST-STAGE CANDIDATES AFTER ALL RETRIEVALS:")
        for c in candidates:
            print(c)
        print()

        candidates = candidates[:top_k]

        # -------------------------------------------------------------
        # 3) Build prompt for ranking LLM
        # -------------------------------------------------------------
        print(">>> STEP 3: Creating LLM prompt")

        tools_lines = []
        for c in candidates:
            aid = c.get("ai_id") or c.get("ID") or ""
            name = (c.get("Name") or "").strip()
            desc = (c.get("Task Description") or "").strip()
            tools_lines.append(f"{aid} | {name} — {desc}")

        tools_text = "\n".join(tools_lines)

        prompt = f"""
You are a tool-ranking assistant. The user request is:
\"\"\"{input_text}\"\"\"

Below is the list of candidate tools (format: ai_id | Name — Task Description):

{tools_text}

Return a JSON array ONLY.
Each element MUST be a JSON OBJECT.
"""

        print("\n===== LLM PROMPT SENT =====")
        print(prompt)
        print("===========================\n")

        # -------------------------------------------------------------
        # 4) CALL OLLAMA
        # -------------------------------------------------------------
        print(">>> STEP 4: Calling Ollama model:", OLLAMA_MODEL)
        try:
            result = subprocess.run(
                ["ollama", "run", OLLAMA_MODEL, prompt],
                capture_output=True, text=True, timeout=TIMEOUT
            )
            raw = (result.stdout or "").strip()
        except Exception as e:
            print("Primary LLM failed:", e)
            print("Trying QUICK_MODEL instead:", QUICK_MODEL)
            try:
                result = subprocess.run(
                    ["ollama", "run", QUICK_MODEL, prompt],
                    capture_output=True, text=True, timeout=TIMEOUT
                )
                raw = (result.stdout or "").strip()
            except Exception as e2:
                print("Both LLM calls failed:", e2)
                return normalize_records(llm_fallback(input_text))

        print("===== RAW LLM OUTPUT =====")
        print(raw)
        print("==========================\n")

        # -------------------------------------------------------------
        # 5) PARSE JSON
        # -------------------------------------------------------------
        print(">>> STEP 5: Parsing JSON output…")
        parsed = extract_json_from_text(raw)

        print("Parsed JSON:", parsed, "\n")

        # fallback if parse failed
        if not parsed:
            print("JSON parsing failed → using fallback ordering")
            for i, c in enumerate(candidates):
                c["score"] = float(c.get("score", 1.0 - i * 0.1))
                c["Reasoning"] = c.get("Task Description", "")[:160]
            return normalize_records(candidates[:top_k])

        if isinstance(parsed, dict):
            parsed = [parsed]
        if not isinstance(parsed, list):
            parsed = []

        # -------------------------------------------------------------
        # 6) MAP LLM RESULTS BACK (⚠ includes FIX)
        # -------------------------------------------------------------
        print(">>> STEP 6: Mapping LLM IDs back to candidate objects")

        csv_candidates = candidates
        by_id = {str((c.get("ai_id") or c.get("ID") or "")).strip(): c for c in csv_candidates}
        by_name = {((c.get("Name") or "").strip().lower()): c for c in csv_candidates}

        print("ID map keys:", list(by_id.keys()))
        print("Name map keys:", list(by_name.keys()), "\n")

        results = []

        for item in parsed:
            print("Processing LLM item:", item)

            # FIX: LLM returned bare number
            if isinstance(item, (int, float, str)):
                print("LLM item is bare number/string → converting")
                item = {
                    "ai_id": str(item),
                    "Name": "",
                    "Score": 1.0,
                    "Reasoning": "LLM returned bare ID"
                }

            try:
                aid = str(item.get("ai_id") or item.get("id") or "").strip()
                name = (item.get("Name") or "").strip()
                score = float(item.get("Score", item.get("score", 0.0)))
                reason = item.get("Reasoning") or ""

                print("Mapped fields:", aid, name, score, reason)

                match = None
                if aid in by_id:
                    print("Matched by ai_id")
                    match = dict(by_id[aid])
                elif name.lower() in by_name:
                    print("Matched by Name")
                    match = dict(by_name[name.lower()])
                else:
                    print("No match found → keeping LLM item")

                if match:
                    match["score"] = score
                    match["Reasoning"] = reason
                    results.append(match)
                else:
                    results.append({
                        "ai_id": aid,
                        "Name": name or "Unknown",
                        "Task Description": reason,
                        "score": score,
                        "Reasoning": reason
                    })

            except Exception as e:
                print("ERROR processing item:", e)

        print("\nMapped results so far:")
        for r in results:
            print(r)

        # -------------------------------------------------------------
        # 7) PAD IF RESULTS FEWER THAN K
        # -------------------------------------------------------------
        seen = {((r.get("Name") or "").strip().lower()) for r in results}

        print("\n>>> STEP 7: Padding missing items if needed")
        for c in csv_candidates:
            if len(results) >= top_k:
                break
            if (c.get("Name") or "").strip().lower() in seen:
                continue
            entry = dict(c)
            entry["score"] = entry.get("score", 0.0)
            entry["Reasoning"] = entry.get("Task Description", "")[:160]
            print("Adding padding item:", entry)
            results.append(entry)

        # -------------------------------------------------------------
        # 8) FINAL SORT
        # -------------------------------------------------------------
        print("\n>>> STEP 8: FINAL SORT BY SCORE")
        results = sorted(results, key=lambda x: x.get("score", 0.0), reverse=True)[:top_k]

        print("\n===== FINAL RERANKED RESULTS =====")
        for r in results:
            print(r)
        print("===================================\n")

        return normalize_records(results)

    except Exception as e:
        print("DEEP SEARCH ERROR:", e)
        return normalize_records([{
            "ai_id": 0,
            "Name": "Deep-Search-Error",
            "Category": "Error",
            "Task Description": str(e),
            "score": 0.0
        }])
